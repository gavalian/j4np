\section{Design}

In nuclear physics experiments, data goes through multiple stages of transformation before reaching the physics analysis phase. Initially, raw signals are collected from the experimental setup, capturing the direct measurements from each detector component. These raw signals are then processed to extract key features, such as pulse shapes and timing information. 

The subsequent step involves integrating the data from individual detector components. Signals are grouped and correlated across detectors to identify matches, enabling the reconstruction of particle trajectories and characteristics. Finally, the reconstructed particles are analyzed to determine their interactions with each detector component, providing the foundation for further physics analysis.

%Conventionally, different data formats are used in different stages of the data lifecycle, which presents several challenges,
%maintaining a few different libraries for reading and writing files and maintaining code for translating between the data formats.
%This is using CPU cycles for just transforming data structures. Also, different data persistence libraries are using different compression algorithms with varying throughput and efficiency. 

Traditionally, different stages of the data lifecycle employ distinct data formats, which introduces several challenges. Maintaining multiple libraries for reading and writing files, as well as developing and maintaining code for translating between these formats, can be resource-intensive. This process consumes valuable CPU cycles solely for transforming data structures.

The use of diverse data persistence libraries, each employing unique compression algorithms, creates inconsistencies in throughput and efficiency. These variations complicate optimization efforts and can negatively impact the overall performance of data processing workflows.

This project aimed to design a unified data format capable of efficiently handling data across all stages of experimental workflows while facilitating the analysis of large datasets. The requirements for the new format were informed by extensive experience with experimental data and include the following:
\begin{itemize}
\item {\bf Compression Efficiency:} The data format must incorporate compression to minimize storage requirements. The chosen compression algorithm should strike a balance between speed and compression ratio. High compression speed is critical due to the large data volumes generated by experimental setups, particularly in high-rate nuclear physics experiments where high data throughput is essential.
\item {\bf Random Access Capability:} The format must support random access to specific data collections within a file. This feature is crucial for debugging, enabling selective writing of data subsets, and for multi-threaded applications that process chunks of data asynchronously.
\item {\bf Data Grouping Functionality:} The format should allow for the grouping of related datasets. This capability is essential for marking or tagging different datasets, enabling targeted reading of specific groups without the need to process the entire dataset.
\end{itemize}
For a more detailed explanation of these features and examples of their implementation, please refer to the following text.
